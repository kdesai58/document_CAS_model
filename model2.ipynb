{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AIZVUvCvqchS",
        "outputId": "01a0d8eb-44eb-434b-ead0-d3924aaf3682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DxaeVM3-qchS"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error extracting text from PDF: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "heJ93oK7qchT",
        "outputId": "1f917f45-4c3b-4463-e2f2-c7fe5108d41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wSTqpfoeqchT",
        "outputId": "0f3abf18-f0a3-45d4-8c9a-82b7ff2c2f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid! Logged in as: khilandesai\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import whoami\n",
        "\n",
        "token = \"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\"\n",
        "\n",
        "try:\n",
        "    user_info = whoami(token)\n",
        "    print(f\"Token is valid! Logged in as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Token validation failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9aOEHJkOqchT"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your_huggingface_token' with the token you generated\n",
        "login(token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RI-WLdlDqchT"
      },
      "outputs": [],
      "source": [
        "# huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OF3q6EXVqchT",
        "outputId": "08c63b51-5ce9-4b62-8cae-2857f2df61d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'accelerate>=0.26.0'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WnLW-T_9qchT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "\n",
        "# accelerator = Accelerator()\n",
        "# Load the tokenizer and model\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"  # Change to the specific LLaMA model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Ensure pad_token_id is set correctly (Llama models may not have a pad_token by default)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Set it to EOS token if it's not available\n",
        "\n",
        "# model = accelerator.prepare(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_aUqSdyuqchT"
      },
      "outputs": [],
      "source": [
        "def classify_text(text, categories):\n",
        "    \"\"\"\n",
        "    Classifies text into one of the given categories using LLaMA.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    The following is a classification task. Classify the given text into one of the following categories:\n",
        "    {\", \".join(categories)}.\n",
        "\n",
        "    Text: {text}\n",
        "\n",
        "    Category:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, padding=True,truncation=True,return_tensors=\"pt\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = inputs.to(device)\n",
        "    model.to(device)  # Move the model to the same device as inputs\n",
        "\n",
        "    # Ensure attention mask is properly set\n",
        "    attention_mask = inputs.get('attention_mask', torch.ones(inputs['input_ids'].shape))\n",
        "\n",
        "    # Run the model to get logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=attention_mask, max_length=100)\n",
        "\n",
        "    # Decode the output and extract the category\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the category by looking after the 'Category:' part\n",
        "    classification = output_text.split('Category:')[-1].strip()\n",
        "\n",
        "    # Return the predicted category\n",
        "    return classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "H7_ljzxJqchT",
        "outputId": "a587c6b6-4ebf-427e-eec5-34b4ff43b343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: Finance\n",
            "\n",
            "    Text: The world's largest financial institution,\n"
          ]
        }
      ],
      "source": [
        "categories = [\"Technology\", \"Finance\" ,\"Healthcare\", \"Education\", \"Arts\", \"General knowledge\", \"Science\", \"Sports\"]\n",
        "text = \"Artificial intelligence is revolutionizing healthcare by improving diagnostics and treatments.\"\n",
        "\n",
        "predicted_category = classify_text(text, categories)\n",
        "print(f\"Predicted Category: {predicted_category}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgJ-AekYqchT"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a classification pipeline\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def classify_text(text):\n",
        "    \"\"\"Classify the text using a Hugging Face model.\"\"\"\n",
        "    result = classifier(text[:1024])  # Process the first 512 tokens for simplicity\n",
        "    return result[0][\"label\"], result[0][\"score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f2psIZoqchT"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-small\")\n",
        "\n",
        "def summarize_text(text):\n",
        "    \"\"\"Summarize the input text.\"\"\"\n",
        "    return summarizer(text[:2048], max_length=100, min_length=30, do_sample=False)[0][\"summary_text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hACtv2AiqchT"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi sentence-transformers faiss-cpu uvicorn nest_asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqMGIqBSqchT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# FAISS Index\n",
        "dimension = 384  # Embedding size of all-MiniLM-L6-v2\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
        "\n",
        "# Database to store metadata\n",
        "document_db = []\n",
        "\n",
        "def add_document_to_index(doc_id, text):\n",
        "    \"\"\"Generate embeddings and add them to FAISS index.\"\"\"\n",
        "    embedding = embedding_model.encode([text])[0]  # Generate embedding\n",
        "    index.add(np.array([embedding]))  # Add to FAISS index\n",
        "    document_db.append({\"id\": doc_id, \"text\": text})  # Add metadata to DB\n",
        "\n",
        "def search_documents(query, top_k=5):\n",
        "    \"\"\"Perform a similarity search using FAISS.\"\"\"\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "    results = [document_db[i] for i in indices[0]]\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcZENidRqchU"
      },
      "outputs": [],
      "source": [
        "!pip install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5jie4QFqchU"
      },
      "outputs": [],
      "source": [
        "# from fastapi import FastAPI\n",
        "\n",
        "# app = FastAPI()\n",
        "\n",
        "# @app.get(\"/\")\n",
        "# def read_root():\n",
        "#     return {\"Hello\": \"World\"}\n",
        "\n",
        "# @app.post(\"/upload/\")\n",
        "# async def upload_file():\n",
        "#     return {\"message\": \"File uploaded!\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1f8mL45qchU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Import necessary modules\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import shutil\n",
        "import uuid\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "UPLOAD_DIR = \"./uploads\"\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# def extract_text_from_pdf(file_path: str) -> str:\n",
        "#     try:\n",
        "#         reader = PdfReader(file_path)\n",
        "#         text = \"\"\n",
        "#         for page in reader.pages:\n",
        "#             text += page.extract_text()\n",
        "#         return text\n",
        "#     except Exception as e:\n",
        "#         raise ValueError(f\"Error extracting text from PDF: {e}\")\n",
        "\n",
        "@app.post(\"/upload/\")\n",
        "async def upload_document(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        doc_id = str(uuid.uuid4())\n",
        "        file_location = os.path.join(UPLOAD_DIR, file.filename)\n",
        "        with open(file_location, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "        text = extract_text_from_pdf(file_location)\n",
        "        # Summarize, classify, and embed\n",
        "        summary = summarize_text(text)\n",
        "        label, score = classify_text(text)\n",
        "        add_document_to_index(doc_id, text)\n",
        "\n",
        "        # Return results\n",
        "        return {\n",
        "            \"id\": doc_id,\n",
        "            \"filename\": file.filename,\n",
        "            \"extracted_text\": text,\n",
        "            \"summary\": summary,\n",
        "            \"classification\": {\"label\": label, \"score\": score}\n",
        "        }\n",
        "        # return {\"message\": \"File uploaded and text extracted successfully!\", \"extracted_text\": text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error: {e}\")\n",
        "\n",
        "@app.get(\"/search/\")\n",
        "async def search_document(query: str):\n",
        "    try:\n",
        "        results = search_documents(query)\n",
        "        return {\"results\": results}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error: {e}\")\n",
        "\n",
        "# Run FastAPI in notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# # Optional: Expose your app to the internet using ngrok\n",
        "# public_url = ngrok.connect(8000)\n",
        "# print(f\"Public URL: {public_url}\")\n",
        "\n",
        "uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c308wMsqchU"
      },
      "outputs": [],
      "source": [
        "# from fastapi import FastAPI, UploadFile, File\n",
        "# import uuid\n",
        "# import shutil\n",
        "\n",
        "# app = FastAPI()\n",
        "\n",
        "# @app.get(\"/\")\n",
        "# def read_root():\n",
        "#     return {\"message\": \"Welcome to the FastAPI app!\"}\n",
        "\n",
        "# @app.post(\"/upload/\")\n",
        "# async def upload_document(file: UploadFile = File(...)):\n",
        "#     \"\"\"Upload and process a document.\"\"\"\n",
        "#     doc_id = str(uuid.uuid4())  # Generate unique ID\n",
        "#     file_path = f\"uploads/{doc_id}_{file.filename}\"\n",
        "\n",
        "#     # Save the uploaded file\n",
        "#     with open(file_path, \"wb\") as buffer:\n",
        "#         shutil.copyfileobj(file.file, buffer)\n",
        "\n",
        "#     # Extract text from the document\n",
        "#     text = extract_text_from_pdf(file_path)\n",
        "\n",
        "#     # Summarize, classify, and embed\n",
        "#     summary = summarize_text(text)\n",
        "#     label, score = classify_text(text)\n",
        "#     add_document_to_index(doc_id, text)\n",
        "\n",
        "#     # Return results\n",
        "#     return {\n",
        "#         \"id\": doc_id,\n",
        "#         \"filename\": file.filename,\n",
        "#         \"summary\": summary,\n",
        "#         \"classification\": {\"label\": label, \"score\": score}\n",
        "#     }\n",
        "\n",
        "# @app.get(\"/search/\")\n",
        "# async def search(query: str):\n",
        "#     \"\"\"Search for documents based on a query.\"\"\"\n",
        "#     results = search_documents(query)\n",
        "#     return {\"query\": query, \"results\": results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb62GBfCqchU"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9sb-4QdqchU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Upload a file\n",
        "url = \"http://0.0.0.0:8000/upload/\"\n",
        "files = {\"file\": open(\"/Users/HP 1/Desktop/Project1/document_CAS_model/0000/0000009.pdf\", \"rb\")}\n",
        "response = requests.post(url, files=files)\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMnRWYqrqchU"
      },
      "outputs": [],
      "source": [
        "# Search for a term\n",
        "url = \"http://127.0.0.1:8000/search/\"\n",
        "params = {\"query\": \"linear regression\"}\n",
        "response = requests.get(url, params=params)\n",
        "print(response.json())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}